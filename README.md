# GUI-Agent-Survey

This is the official repository for the paper "Empowering GUI Agent with Large Language Model: A Comprehensive Survey".

## Abstract

The Graphical User Interface (GUI) is a visual method that allows users to interact with computers and mobile electronic devices. Nowadays users rely on GUI for completing some tasks such as browsing web or using mobile applications. Users often face the challenge to handle tedious tasks within GUI, such as replying to a specific type mass emails in Gmail application. GUI Agent are designed to help users by automating tasks in the GUI environment. Traditional automation agents lack flexibility, are often limited to specific applications, and require significant manual intervention. Benefiting from the powerful planning and reasoning capabilities of Large Language Model (LLM), LLM-based Agent achieved significant success in some fields (healthcare and autonomous driving). Researchers have also begun to explore the potential of LLM-based GUI Agent to automate task. Fortunately, these GUI agents have overcome the limitations of traditional agents and demonstrated remarkable performance.  Our survey reviews recent relevant LLM-based GUI Agent research and defines GUI task automation flow,  we focus on three key issues : (1) GUI Environment Understanding;  (2) User Interaction Experience;  (3) Efficient automation task Key step.  We finally outline the challenge and future opportunity in this field. This paper aims to provide valuable insights for researchers and engineers. A comprehensive list of studies in this survey is available at [https://github.com/longzhaohuang/GUI-Agent-Survey](https://github.com/longzhaohuang/GUI-Agent-Survey).

## News

ðŸ˜Š This project is under development. You can hit the **STAR** and **WATCH** to follow the updates.

## Table of Contents

- [GUI-Agent-Survey](#GUI-Agent-Survey)
  - [Abstract](#abstract)
  - [News](#news)
  - [Table of Contents](#table-of-contents)
  - [Related Paper](#related-paper)
  - [GUI Environment Understanding](#GUI-Environment-Understanding)
    - [Text-Based Understanding Method](#Text-Based-Understanding-Method)
    - [Vision-Based Understanding Method](#Vision-Based-Understanding-Method)
    - [Hybrid Text-Vision Understanding Method](#Hybrid-Text-Vision-Understanding-Method)
  - [Device Control](#Device-Control)
    - [Code-Based Method](#Code-Based-Method)
    - [Ui-Based Method](#Ui-Based-Method)
  - [Dataset](#Dataset)
  - [Acknowledgement](#acknowledgement)

## Related Paper
* Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security
 [[paper]](https://arxiv.org/abs/2401.05459)
* The Rise and Potential of Large Language Model Based Agents: A Survey [[paper]](https://arxiv.org/abs/2309.07864)
* Large Language Model based Multi-Agents: A Survey of Progress and Challenges [[paper]](https://arxiv.org/abs/2402.01680)

## GUI Environment Understanding
### Text-Based Understanding Method
* Explore, Select, Derive, and Recall: Augmenting LLM with Human-like Memory for Mobile Task Automation [[paper]](https://arxiv.org/pdf/2312.03003)
* DroidBot-GPT: GPT-powered UI Automation for Android [[paper]](https://arxiv.org/pdf/2304.07061) 
* Enabling Conversational Interaction with Mobile UI using Large Language Models [[paper]](https://arxiv.org/abs/2209.08655)
* Mind2Web: Towards a Generalist Agent for the Web [[paper]](https://arxiv.org/abs/2306.06070)
* A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis [[paper]](https://arxiv.org/abs/2307.12856)
* WebGPT: Browser-assisted question-answering with human feedback [[paper]](https://arxiv.org/abs/2112.09332)
* Language Models can Solve Computer Tasks [[paper]](https://arxiv.org/abs/2303.17491)
* Adaplanner: Adaptive planning from feedback with language model[[paper]](https://arxiv.org/pdf/2305.16653)

### Vision-Based Understanding Method
* GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation [[paper]](https://arxiv.org/abs/2311.07562)
* WebWISE: Web Interface Control and Sequential Exploration with Large Language Models
[[paper]](https://arxiv.org/pdf/2310.16042)
* Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus [[paper]](https://arxiv.org/abs/2209.14927)
* META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI [[paper]](https://arxiv.org/pdf/2205.11029)
* You Only Look at Screens: Multimodal Chain-of-Action Agents [[paper]](https://arxiv.org/abs/2309.11436)
* Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception[[paper]](https://arxiv.org/pdf/2401.16158)
* Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration [[paper]](https://arxiv.org/pdf/2406.01014)
* SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents [[paper]](https://arxiv.org/abs/2401.10935)
* CoCo-Agent: A Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation[[paper]](https://arxiv.org/pdf/2402.11941)
* Cogagent: A visual language model for gui agents[[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Hong_CogAgent_A_Visual_Language_Model_for_GUI_Agents_CVPR_2024_paper.pdf)
* UFO: A UI-Focused Agent for Windows OS Interaction [[paper]](https://arxiv.org/pdf/2402.07939)
* MMAC-Copilot: Multi-modal Agent Collaboration Operating System Copilot [[paper]](https://arxiv.org/abs/2404.18074)

### Hybrid Text-Vision Understanding Method
* AppAgent: Multimodal Agents as Smartphone Users [[paper]](https://arxiv.org/abs/2312.13771)
* Multimodal Web Navigation with Instruction-Finetuned Foundation Models [[paper]](https://arxiv.org/abs/2307.12856)
* GPT-4V(ision) is a Generalist Web Agent, if Grounded[[paper]](https://arxiv.org/abs/2401.01614)
* Dual-View Visual Contextualization for Web Navigation [[paper]](https://arxiv.org/abs/2402.04476)
* WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models [[paper]](https://arxiv.org/abs/2401.13919)
* Do BERTs Learn to Use Browser User Interface? Exploring Multi-Step Tasks with Unified Vision-and-Language BERTs [[paper]](https://arxiv.org/pdf/2203.07828)
* AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web Navigating Agent [[paper]](https://arxiv.org/pdf/2404.03648)

## Device Control
### Code-Based Method
[24]* [[paper]]()
[75]* [[paper]]()
[77]* [[paper]]()
[81]* [[paper]]()
[82]* [[paper]]()
[83]* [[paper]]()

### Ui-Based Method
[62]* [[paper]]()
[63]* [[paper]]()
[64]* [[paper]]()
[65]* [[paper]]()
[66]* [[paper]]()
[67]* [[paper]]()
[68]* [[paper]]()
[69]* [[paper]]()
[70]* [[paper]]()
[71]* [[paper]]()
[72]* [[paper]]()
[73]* [[paper]]()
[74]* [[paper]]()
[78]* [[paper]]()
[79]* [[paper]]()
[80]* [[paper]]()
[83]* [[paper]]()

## Dataset
[11]* [[paper]]()
[13]* [[paper]]()
[79]* [[paper]]()
[63]* [[paper]]()
[64]* [[paper]]()
[65]* [[paper]]()
[69]* [[paper]]()
[70]* [[paper]]()
[71]* [[paper]]()
[72]* [[paper]]()
[73]* [[paper]]()
[74]* [[paper]]()
[107]* [[paper]]()
[108]* [[paper]]()
[113]* [[paper]]()
[114]* [[paper]]()
[115]* [[paper]]()
[116]* [[paper]]()
[117]* [[paper]]()
[118]* [[paper]]()
[119]* [[paper]]()
[120]* [[paper]]()
[121]* [[paper]]()
[122]* [[paper]]()
[123]* [[paper]]()
[124]* [[paper]]()


## Acknowledgement

This work is supported by Beijing Natural Science Foundation No. JQ23014, in part by the National Natural Science Foundation of China (No. 62271074).
