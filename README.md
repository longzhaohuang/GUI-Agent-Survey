# GUI-Agent-Survey

This is the official repository for the paper "Empowering GUI Agent with Large Language Model: A Comprehensive Survey".

## Abstract

The Graphical User Interface (GUI) is a visual method that allows users to interact with computers and mobile electronic devices. Nowadays users rely on GUI for completing some tasks such as browsing web or using mobile applications. Users often face the challenge to handle tedious tasks within GUI, such as replying to a specific type mass emails in Gmail application. GUI Agent are designed to help users by automating tasks in the GUI environment. Traditional automation agents lack flexibility, are often limited to specific applications, and require significant manual intervention. Benefiting from the powerful planning and reasoning capabilities of Large Language Model (LLM), LLM-based Agent achieved significant success in some fields (healthcare and autonomous driving). Researchers have also begun to explore the potential of LLM-based GUI Agent to automate task. Fortunately, these GUI agents have overcome the limitations of traditional agents and demonstrated remarkable performance.  Our survey reviews recent relevant LLM-based GUI Agent research and defines GUI task automation flow,  we focus on three key issues : (1) GUI Environment Understanding;  (2) User Interaction Experience;  (3) Efficient automation task Key step.  We finally outline the challenge and future opportunity in this field. This paper aims to provide valuable insights for researchers and engineers. A comprehensive list of studies in this survey is available at [https://github.com/longzhaohuang/GUI-Agent-Survey](https://github.com/longzhaohuang/GUI-Agent-Survey).

## News

ðŸ˜Š This project is under development. You can hit the **STAR** and **WATCH** to follow the updates.

## Table of Contents

- [GUI-Agent-Survey](#GUI-Agent-Survey)
  - [Abstract](#abstract)
  - [News](#news)
  - [Table of Contents](#table-of-contents)
  - [GUI Environment Understanding](#GUI-Environment-Understanding)
    - [Text-Based Understanding Method](#Text-Based-Understanding-Method)
    - [Vision-Based Understanding Method](#Vision-Based-Understanding-Method)
    - [Hybrid Text-Vision Understanding Method](#Hybrid-Text-Vision-Understanding-Method)
  - [Device Control](#Device-Control)
    - [Code-Based Method](#Code-Based-Method)
    - [Ui-Based Method](#Ui-Based-Method)
  - [Dataset](#Dataset)
  - [Acknowledgement](#acknowledgement)

## GUI Environment Understanding
### Text-Based Understanding Method
[65]* Explore, Select, Derive, and Recall: Augmenting LLM with Human-like Memory for Mobile Task Automation [[paper]](https://arxiv.org/pdf/2312.03003)
[67]* [[paper]]() 
[84]* [[paper]]()
[74]* [[paper]]()
[75]* [[paper]]()

### Vision-Based Understanding Method
[23]* GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation [[paper]]()
[24]* [[paper]]()
[62]* Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus [[paper]](https://arxiv.org/abs/2209.14927)
[63]* META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI [[paper]](https://arxiv.org/pdf/2205.11029)
[64]* You Only Look at Screens: Multimodal Chain-of-Action Agents [[paper]](https://arxiv.org/abs/2309.11436)
[69]* [[paper]]()
[72]* [[paper]]()
[78]* [[paper]]()
[98]* [[paper]]()

### Hybrid Text-Vision Understanding Method
[66]* AppAgent: Multimodal Agents as Smartphone Users [[paper]](https://arxiv.org/abs/2312.13771)
[73]* [[paper]]()
[80]* [[paper]]()
[79]* [[paper]]()
[99]* [[paper]]()

## Device Control
### Code-Based Method
[24]* [[paper]]()
[75]* [[paper]]()
[77]* [[paper]]()
[81]* [[paper]]()
[82]* [[paper]]()
[83]* [[paper]]()

### Ui-Based Method
[62]* [[paper]]()
[63]* [[paper]]()
[64]* [[paper]]()
[65]* [[paper]]()
[66]* [[paper]]()
[67]* [[paper]]()
[68]* [[paper]]()
[69]* [[paper]]()
[70]* [[paper]]()
[71]* [[paper]]()
[72]* [[paper]]()
[73]* [[paper]]()
[74]* [[paper]]()
[78]* [[paper]]()
[79]* [[paper]]()
[80]* [[paper]]()
[83]* [[paper]]()

## Dataset
[11]* [[paper]]()
[13]* [[paper]]()
[79]* [[paper]]()
[63]* [[paper]]()
[64]* [[paper]]()
[65]* [[paper]]()
[69]* [[paper]]()
[70]* [[paper]]()
[71]* [[paper]]()
[72]* [[paper]]()
[73]* [[paper]]()
[74]* [[paper]]()
[107]* [[paper]]()
[108]* [[paper]]()
[113]* [[paper]]()
[114]* [[paper]]()
[115]* [[paper]]()
[116]* [[paper]]()
[117]* [[paper]]()
[118]* [[paper]]()
[119]* [[paper]]()
[120]* [[paper]]()
[121]* [[paper]]()
[122]* [[paper]]()
[123]* [[paper]]()
[124]* [[paper]]()


## Acknowledgement

This work is supported by Beijing Natural Science Foundation No. JQ23014, in part by the National Natural Science Foundation of China (No. 62271074).
